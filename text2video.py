# -*- coding: utf-8 -*-
"""Text2Video.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ojYcmzpdvsfRvMARqvjPFa753sWMaruc
"""

from google.colab import drive
drive.mount('/content/drive')

import os
video_folder = '/content/drive/My Drive/datasets'

# List all files in the dataset folder
video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]

for video in video_files:
    video_path = os.path.join(video_folder, video)
    print(f"Loading video: {video_path}")
    # Load and process the video
    # Example: Load with OpenCV or other tools as needed

json_path = '/content/drive/My Drive/your_folder_name/VidGen_1M_video_caption.json'  # Replace with the exact filename

import os

# List all files in the datasets folder
folder_path = '/content/drive/My Drive/datasets'
files = os.listdir(folder_path)
print(files)  # This will show you the exact names of the files in the folder

import os

# Set the path to your datasets folder
dataset_path = '/content/drive/My Drive/datasets'

# Count .mp4 files
mp4_files = [f for f in os.listdir(dataset_path) if f.endswith('.mp4')]
mp4_count = len(mp4_files)

print(f"Total number of .mp4 files: {mp4_count}")

import os
from collections import Counter

# Path to your datasets folder
dataset_path = '/content/drive/My Drive/datasets'

# List all .mp4 files
mp4_files = [f for f in os.listdir(dataset_path) if f.endswith('.mp4')]

# Count occurrences of each file name
file_count = Counter(mp4_files)

# Identify duplicates
duplicates = {file: count for file, count in file_count.items() if count > 1}

if duplicates:
    print("Duplicate files found:")
    for file, count in duplicates.items():
        print(f"{file} - {count} copies")
else:
    print("No duplicate files found.")

!pip install opencv-python-headless moviepy

import json

# Path to your JSON file in Google Drive
json_path = '/content/drive/My Drive/datasets/VidGen_1M_video_caption.json'

# Load the JSON data
with open(json_path, 'r') as f:
    json_data = json.load(f)

# Print the first few items to inspect the structure
print("Sample JSON data:")
for item in json_data[:3]:  # Print first 3 items
    print(item)

import os
import cv2

# Path to your datasets folder
dataset_path = '/content/drive/My Drive/datasets'

# List all .mp4 files in the folder
mp4_files = [f for f in os.listdir(dataset_path) if f.endswith('.mp4')]

# Prepare a list to store the output in the desired format
video_data = []

# Loop through each file and gather details
for video_file in mp4_files:
    try:
        file_path = os.path.join(dataset_path, video_file)

        # Get file size
        file_size = os.path.getsize(file_path)

        # Use cv2 to get video properties
        video = cv2.VideoCapture(file_path)
        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = video.get(cv2.CAP_PROP_FPS)
        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))

        # Calculate duration
        duration = frame_count / fps if fps else 0

        # Extract video ID from the filename (without the extension)
        video_id = os.path.splitext(video_file)[0]

        # Placeholder for the sample caption (replace this with actual caption lookup if available)
        sample_caption = f"This is a sample caption for the video '{video_id}'."

        # Create a dictionary for the video data
        video_info = {
            'vid': video_id,
            'caption': sample_caption,
            'size': f"{file_size} bytes",
            'dimensions': f"{width}x{height} pixels",
            'duration': f"{duration:.2f} seconds"
        }

        # Append to the list
        video_data.append(video_info)

        # Release the video file after processing
        video.release()

    except Exception as e:
        print(f"Error processing {video_file}: {e}")

# Print the formatted video data
for item in video_data:
    print(item)

import json

# Path to your JSON file in Google Drive
json_path = '/content/drive/My Drive/datasets/VidGen_1M_video_caption.json'

# Load the JSON data
with open(json_path, 'r') as f:
    json_data = json.load(f)

# Get the number of training samples
num_samples = len(json_data)

print(f"Total number of training samples: {num_samples}")

import json
import os
import cv2

# Path to your JSON file and datasets folder
json_path = '/content/drive/My Drive/datasets/VidGen_1M_video_caption.json'
dataset_path = '/content/drive/My Drive/datasets'

# Load JSON data
with open(json_path, 'r') as f:
    json_data = json.load(f)

# Create a dictionary to map video IDs to captions for easy lookup
caption_dict = {item['vid']: item['caption'] for item in json_data}

# List all .mp4 files in the folder
mp4_files = [f for f in os.listdir(dataset_path) if f.endswith('.mp4')]

# Prepare a list to store the output in the desired format
video_data = []

# Loop through each file and gather details
for video_file in mp4_files:
    try:
        file_path = os.path.join(dataset_path, video_file)

        # Get file size
        file_size = os.path.getsize(file_path)

        # Use cv2 to get video properties
        video = cv2.VideoCapture(file_path)
        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = video.get(cv2.CAP_PROP_FPS)
        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))

        # Calculate duration
        duration = frame_count / fps if fps else 0

        # Extract video ID from the filename (without the extension)
        video_id = os.path.splitext(video_file)[0]

        # Get the actual caption from the JSON data if available
        sample_caption = caption_dict.get(video_id, "No caption available.")

        # Create a dictionary for the video data
        video_info = {
            'vid': video_id,
            'caption': sample_caption,
            'size': f"{file_size} bytes",
            'dimensions': f"{width}x{height} pixels",
            'duration': f"{duration:.2f} seconds"
        }

        # Append to the list
        video_data.append(video_info)

        # Release the video file after processing
        video.release()

    except Exception as e:
        print(f"Error processing {video_file}: {e}")

# Print the formatted video data
for item in video_data:
    print(item)

import json

# Path to save the output JSON file
output_json_path = '/content/drive/My Drive/datasets/video_data_with_captions.json'

# Save the video_data list to a JSON file
with open(output_json_path, 'w') as f:
    json.dump(video_data, f, indent=4)

print(f"Data successfully saved to {output_json_path}")

import json

# Path to the saved JSON file
input_json_path = '/content/drive/My Drive/datasets/video_data_with_captions.json'

# Load the JSON data
with open(input_json_path, 'r') as f:
    video_data = json.load(f)

# Print the first few items to verify
print("Sample video data loaded from JSON:")
for item in video_data[:3]:
    print(item)

total_duration = sum(float(video['duration'].split()[0]) for video in video_data)
average_duration = total_duration / len(video_data)
print(f"Average video duration: {average_duration:.2f} seconds")

size_ranges = {"<1MB": 0, "1MB-10MB": 0, ">10MB": 0}

for video in video_data:
    size_in_bytes = int(video['size'].split()[0])
    size_in_mb = size_in_bytes / (1024 * 1024)

    if size_in_mb < 1:
        size_ranges["<1MB"] += 1
    elif size_in_mb <= 10:
        size_ranges["1MB-10MB"] += 1
    else:
        size_ranges[">10MB"] += 1

print("Video count by size range:")
for range_label, count in size_ranges.items():
    print(f"{range_label}: {count}")

def search_videos(keyword):
    results = [video for video in video_data if keyword.lower() in video['caption'].lower()]
    return results

# Example: Search for videos with "flower" in the caption
keyword = "flower"
found_videos = search_videos(keyword)
print(f"Videos containing '{keyword}' in caption:")
for video in found_videos:
    print(video)

import pandas as pd
import json

# Load JSON data into a DataFrame
input_json_path = '/content/drive/My Drive/datasets/video_data_with_captions.json'
with open(input_json_path, 'r') as f:
    video_data = json.load(f)

# Convert to DataFrame
video_df = pd.DataFrame(video_data)

# Convert duration to numeric by stripping 'seconds' and converting to float
video_df['duration_sec'] = video_df['duration'].str.replace(' seconds', '').astype(float)

# Show basic statistics for numeric columns
print("Video Duration and Size Statistics:")
print(video_df[['duration_sec']].describe())

import matplotlib.pyplot as plt
import seaborn as sns

# Plot a histogram of video durations
plt.figure(figsize=(10, 6))
sns.histplot(video_df['duration_sec'], bins=20, kde=True, color='skyblue')
plt.title("Distribution of Video Durations")
plt.xlabel("Duration (seconds)")
plt.ylabel("Frequency")
plt.show()

# Convert size to megabytes
video_df['size_mb'] = video_df['size'].str.replace(' bytes', '').astype(float) / (1024 * 1024)

# Plot the distribution of video sizes
plt.figure(figsize=(10, 6))
sns.histplot(video_df['size_mb'], bins=20, kde=True, color='lightgreen')
plt.title("Distribution of Video File Sizes")
plt.xlabel("Size (MB)")
plt.ylabel("Frequency")
plt.show()

# Count the unique resolutions
resolution_counts = video_df['dimensions'].value_counts()

# Plot the count of videos by resolution
plt.figure(figsize=(12, 6))
resolution_counts.plot(kind='bar', color='salmon')
plt.title("Count of Videos by Resolution")
plt.xlabel("Resolution")
plt.ylabel("Number of Videos")
plt.xticks(rotation=45)
plt.show()

# Scatter plot of video size vs. duration
plt.figure(figsize=(10, 6))
sns.scatterplot(data=video_df, x='size_mb', y='duration_sec', color='purple')
plt.title("Video Size vs. Duration")
plt.xlabel("Size (MB)")
plt.ylabel("Duration (seconds)")
plt.show()

from wordcloud import WordCloud

# Combine all captions into a single text
all_captions = ' '.join(video_df['caption'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_captions)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Define categories for duration
video_df['length_category'] = pd.cut(video_df['duration_sec'], bins=[0, 10, 30, 300], labels=["Short", "Medium", "Long"])

# Plot count of each category
plt.figure(figsize=(8, 6))
sns.countplot(data=video_df, x='length_category', palette="viridis")
plt.title("Video Count by Length Category")
plt.xlabel("Length Category")
plt.ylabel("Number of Videos")
plt.show()

# Calculate average duration for each resolution
avg_duration_by_res = video_df.groupby('dimensions')['duration_sec'].mean().sort_values()

# Plot average duration by resolution
plt.figure(figsize=(12, 6))
avg_duration_by_res.plot(kind='barh', color='coral')
plt.title("Average Video Duration by Resolution")
plt.xlabel("Average Duration (seconds)")
plt.ylabel("Resolution")
plt.show()

processed_csv_path = '/content/drive/My Drive/datasets/processed_video_data_with_insights.csv'
video_df.to_csv(processed_csv_path, index=False)
print(f"Processed data with insights saved to {processed_csv_path}")

import pandas as pd

# Convert the video data to a DataFrame
video_df = pd.DataFrame(video_data)

# Display the first few rows
print("Video Data DataFrame:")
print(video_df.head())

# Convert duration column to numeric for analysis
video_df['duration_sec'] = video_df['duration'].str.replace(' seconds', '').astype(float)

# Calculate statistics
print("Video Duration Statistics:")
print(video_df['duration_sec'].describe())

# Count the unique resolutions
resolution_counts = video_df['dimensions'].value_counts()

print("Resolution Counts:")
print(resolution_counts)

# Find the video with the longest duration
longest_video = video_df.loc[video_df['duration_sec'].idxmax()]
print("\nLongest Video:")
print(longest_video)

# Find the video with the shortest duration
shortest_video = video_df.loc[video_df['duration_sec'].idxmin()]
print("\nShortest Video:")
print(shortest_video)

# Step 1: Check the columns in the DataFrame
print("Columns in video_df:")
print(video_df.columns)

# Step 2: Display the first few rows to understand the structure
print("Sample data from video_df:")
print(video_df.head())

# Step 3: Ensure that width and height are correctly extracted
if 'dimensions' in video_df.columns:
    # Print the unique values in the dimensions column to verify extraction
    print("Unique dimensions values:")
    print(video_df['dimensions'].unique())
else:
    print("Warning: 'dimensions' column is not present in video_df.")

import pandas as pd
import re

# Define the function to extract width and height
def extract_dimensions(dim_str):
    # Check if the string matches the pattern of 'width x height'
    match = re.match(r'(\d+)x(\d+)', dim_str)
    if match:
        return (float(match.group(1)), float(match.group(2)))
    else:
        return (None, None)  # Return None for both if no match is found

# Check the original dimensions
print("Original dimensions:")
print(video_df['dimensions'].head())  # Print first few rows for debugging

# Apply the extraction function to the dimensions column
video_df[['width', 'height']] = video_df['dimensions'].apply(lambda x: extract_dimensions(x)).apply(pd.Series)

# Check for missing values in width and height
print("\nMissing values in width and height:")
print(video_df[['width', 'height']].isnull().sum())

# Now print the updated DataFrame's columns and a sample of its data
print("\nUpdated DataFrame columns:")
print(video_df.columns.tolist())
print("\nSample data from updated video_df:")
print(video_df.head())

# Descriptive statistics for numerical columns
print("Descriptive statistics:")
print(video_df[['size', 'duration_sec', 'width', 'height']].describe())

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for video duration
plt.figure(figsize=(10, 6))
sns.histplot(video_df['duration_sec'], bins=30, kde=True)
plt.title('Distribution of Video Duration')
plt.xlabel('Duration (seconds)')
plt.ylabel('Frequency')
plt.show()

# Scatter plot for width vs. height
plt.figure(figsize=(10, 6))
sns.scatterplot(data=video_df, x='width', y='height')
plt.title('Scatter Plot of Video Width vs Height')
plt.xlabel('Width (pixels)')
plt.ylabel('Height (pixels)')
plt.show()

# Function to convert size from bytes to megabytes
def convert_size_to_mb(size_str):
    # Extract the numeric part from the string
    numeric_part = ''.join(filter(str.isdigit, size_str))  # Keep only digits
    return float(numeric_part) / (1024 * 1024)  # Convert bytes to MB

# Apply the conversion function to the size column
video_df['size_mb'] = video_df['size'].apply(convert_size_to_mb)

# Check the first few entries to ensure conversion
print(video_df[['size', 'size_mb']].head())

# Create a correlation matrix and plot it
correlation_matrix = video_df[['duration_sec', 'size_mb', 'width', 'height']].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

# Check for duplicate entries based on 'vid' or other relevant columns
duplicates = video_df[video_df.duplicated(subset=['vid'], keep=False)]
print(f"Number of duplicate entries: {duplicates.shape[0]}")
print("Duplicate entries:")
print(duplicates)

print(video_df.describe())

plt.figure(figsize=(12, 6))

# Histogram of video durations
plt.subplot(1, 3, 1)
sns.histplot(video_df['duration_sec'], bins=30, kde=True)
plt.title('Distribution of Video Durations')
plt.xlabel('Duration (seconds)')

# Histogram of sizes
plt.subplot(1, 3, 2)
sns.histplot(video_df['size_mb'], bins=30, kde=True)
plt.title('Distribution of Video Sizes')
plt.xlabel('Size (MB)')

# Histogram of width
plt.subplot(1, 3, 3)
sns.histplot(video_df['width'], bins=30, kde=True)
plt.title('Distribution of Video Width')
plt.xlabel('Width (pixels)')

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=video_df, x='size_mb', y='duration_sec')
plt.title('Video Size vs. Duration')
plt.xlabel('Size (MB)')
plt.ylabel('Duration (seconds)')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='size_mb', data=video_df)
plt.title('Boxplot of Video Sizes')
plt.xlabel('Size (MB)')
plt.show()

sns.pairplot(video_df[['duration_sec', 'size_mb', 'width', 'height']])
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=video_df, x='width', y='duration_sec', hue='size_mb', size='size_mb', sizes=(20, 200), alpha=0.5)
plt.title('Video Width vs. Duration (Size Indicated by Size of Points)')
plt.xlabel('Width (pixels)')
plt.ylabel('Duration (seconds)')
plt.legend(title='Size (MB)')
plt.show()

# Detect outliers using IQR for duration
Q1 = video_df['duration_sec'].quantile(0.25)
Q3 = video_df['duration_sec'].quantile(0.75)
IQR = Q3 - Q1

outliers_duration = video_df[(video_df['duration_sec'] < (Q1 - 1.5 * IQR)) | (video_df['duration_sec'] > (Q3 + 1.5 * IQR))]

print(f'Number of outliers in duration: {outliers_duration.shape[0]}')

# Remove duplicates
video_df = video_df.drop_duplicates()

# Check for missing values
print("Missing values in each column:")
print(video_df.isnull().sum())

# Optionally drop rows with missing values
video_df = video_df.dropna()  # or use video_df.fillna(method='ffill') to fill them

from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenize captions
video_df['tokenized_caption'] = video_df['caption'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

# You can also use Label Encoding for categorical features if needed
label_encoder = LabelEncoder()
video_df['vid_encoded'] = label_encoder.fit_transform(video_df['vid'])

from sklearn.preprocessing import StandardScaler

# Example: Normalize size (convert bytes to megabytes first)
video_df['size_mb'] = video_df['size'].str.replace(' bytes', '').astype(float) / (1024 * 1024)

# Standardize features
scaler = StandardScaler()
video_df[['duration_sec', 'size_mb']] = scaler.fit_transform(video_df[['duration_sec', 'size_mb']])

!pip install -U albumentations

# Example of data augmentation (e.g., random cropping)
import albumentations as A

def augment_frame(frame):
    transform = A.Compose([
        A.RandomCrop(width=256, height=256),
        A.HorizontalFlip(p=0.5),
    ])
    return transform(image=frame)['image']

from sklearn.model_selection import train_test_split

# Split the dataset
train_df, test_df = train_test_split(video_df, test_size=0.2, random_state=42)
val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

print(f'Training set size: {len(train_df)}')
print(f'Validation set size: {len(val_df)}')
print(f'Test set size: {len(test_df)}')

import albumentations as A

# Define an augmentation pipeline
transform = A.Compose([
    A.RandomCrop(width=256, height=256),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.HueSaturationValue(p=0.2),
    A.Rotate(limit=30, p=0.5),
])

# Example usage on a single frame
def augment_frame(frame):
    augmented = transform(image=frame)
    return augmented['image']

import cv2

def normalize_frame(frame):
    frame = frame / 255.0  # Normalize to [0, 1]
    return frame

from torch.utils.data import Dataset

class VideoCaptionDataset(Dataset):
    def __init__(self, video_paths, captions, transform=None):
        self.video_paths = video_paths
        self.captions = captions
        self.transform = transform

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        # Load video frames
        frames = self.load_video(self.video_paths[idx])

        if self.transform:
            frames = [self.transform(frame) for frame in frames]

        caption = self.captions[idx]
        return frames, caption

    def load_video(self, path):
        # Implement logic to load frames from video
        pass

import cv2
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50

# Load the pre-trained ResNet model
model = resnet50(pretrained=True)
model.eval()  # Set the model to evaluation mode

# Define the transformation to resize and normalize the frames
preprocess = transforms.Compose([
    transforms.ToPILImage(),          # Convert frame to PIL Image
    transforms.Resize((224, 224)),   # Resize to the input size of the model
    transforms.ToTensor(),            # Convert to Tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize
])

def extract_frame_features(video_path):
    video = cv2.VideoCapture(video_path)
    features = []

    while True:
        ret, frame = video.read()
        if not ret:  # Break the loop if no frame is captured
            break

        # Preprocess the frame
        tensor = preprocess(frame)
        tensor = tensor.unsqueeze(0)  # Add batch dimension

        # Extract features
        with torch.no_grad():
            feature = model(tensor)
            features.append(feature.numpy())  # Append features to list

    video.release()
    return features

# Example usage
video_path = 'path/to/your/video.mp4'
features = extract_frame_features(video_path)
print(f'Extracted {len(features)} feature vectors from video.')

import albumentations as A
import numpy as np

# Define a video augmentation pipeline
augmentation = A.Compose([
    A.RandomCrop(width=200, height=200),
    A.HorizontalFlip(),
    A.Rotate(limit=20),
])

def augment_frame(frame):
    augmented = augmentation(image=frame)['image']
    return augmented

def augment_video(video_path):
    video = cv2.VideoCapture(video_path)
    augmented_frames = []

    while True:
        ret, frame = video.read()
        if not ret:
            break

        # Augment the frame
        augmented_frame = augment_frame(frame)
        augmented_frames.append(augmented_frame)

    video.release()
    return augmented_frames

# Example usage
augmented_frames = augment_video(video_path)
print(f'Augmented {len(augmented_frames)} frames from video.')

# Example of one-hot encoding for dimensions
dimensions_encoded = pd.get_dummies(video_df['dimensions'], prefix='dim')
video_df = pd.concat([video_df, dimensions_encoded], axis=1)

# Drop the original dimensions column if no longer needed
video_df.drop(columns=['dimensions'], inplace=True)

# Function to convert size from bytes to megabytes
def convert_size_to_mb(size_str):
    # Extract the numerical part and convert to float
    return float(size_str.split()[0]) / (1024 * 1024)  # Convert bytes to MB

# Apply the conversion to the 'size' column
video_df['size_mb'] = video_df['size'].apply(convert_size_to_mb)

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Normalize the 'size_mb' and 'duration_sec' columns
video_df[['size_mb', 'duration_sec']] = scaler.fit_transform(video_df[['size_mb', 'duration_sec']])

# Check the columns in video_df
print(video_df.columns.tolist())

# Define features and target
X = video_df.drop(columns=['vid', 'caption', 'size', 'tokenized_caption'], errors='ignore')  # Drop columns that exist
y = video_df['tokenized_caption'] if 'tokenized_caption' in video_df.columns else None  # Check if target exists

# Check for valid y
if y is None:
    print("The target variable 'tokenized_caption' is not found.")
else:
    # Proceed with splitting only if y is valid
    from sklearn.model_selection import train_test_split
    # Split the data: 80% train, 10% validation, 10% test
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # Check the shapes of the splits
    print(f"Training set size: {X_train.shape[0]}")
    print(f"Validation set size: {X_val.shape[0]}")
    print(f"Test set size: {X_test.shape[0]}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load a pre-trained tokenizer and model
model_name = 'gpt2'  # or any other transformer model you prefer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Move model to the appropriate device (GPU if available)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

print(video_df.columns)

print(video_df.head())

# Define features and target
X = video_df.drop(columns=['vid', 'size', 'tokenized_caption'])  # Removed 'cleaned_caption'
y = video_df['tokenized_caption']  # Assuming you're predicting the caption tokens

# Define features and target
X = video_df.drop(columns=['vid', 'size', 'tokenized_caption'])  # Adjust to existing columns
y = video_df['tokenized_caption']  # Using tokenized caption as the target

# Step 1: Print column names to confirm
print(video_df.columns)

# Step 2: Use the correct column names
# Check if 'caption' exists
if 'caption' in video_df.columns:
    X = video_df.drop(columns=['vid', 'size', 'tokenized_caption'])  # Adjust if necessary
    y = video_df['caption']  # Set 'caption' as target if exists
else:
    print("Column 'caption' does not exist in the DataFrame.")